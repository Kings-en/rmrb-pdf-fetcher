# rmrb-crawler ğŸ“°

> ä¸€ä¸ªè‡ªåŠ¨çˆ¬å– **äººæ°‘æ—¥æŠ¥ç”µå­ç‰ˆ (https://paper.people.com.cn/rmrb/)** å½“å¤©æ‰€æœ‰ç‰ˆé¢æ–‡ç« çš„ Python è„šæœ¬ã€‚  
æ”¯æŒ **å¤šçº¿ç¨‹åŠ é€Ÿä¸‹è½½**ï¼Œå¹¶å°†æ‰€æœ‰æ–‡ç« æ­£æ–‡åˆå¹¶ä¿å­˜ä¸ºå•ä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
rmrb-crawler/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ rmrb_crawler.py
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE
```


---

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- è‡ªåŠ¨è¯†åˆ«å½“å¤©æ—¥æœŸï¼ˆæ— éœ€æ‰‹åŠ¨ä¿®æ”¹ï¼‰
- æŠ“å–å½“å¤©å…¨éƒ¨ç‰ˆé¢ï¼ˆnode_01ã€node_02ã€...ï¼‰
- è‡ªåŠ¨è§£ææ¯ç¯‡æ–‡ç« é“¾æ¥ä¸æ­£æ–‡å†…å®¹
- å¤šçº¿ç¨‹åŠ é€Ÿä¸‹è½½ï¼Œæé«˜çˆ¬å–é€Ÿåº¦
- åˆå¹¶ä¿å­˜ä¸º `rmrb_YYYYMMDD.txt` æ–‡ä»¶

---

## ğŸ“¦ ç¯å¢ƒè¦æ±‚

Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬

å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
````

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

1. å…‹éš†é¡¹ç›®ï¼š

   ```bash
   git clone https://github.com/ä½ çš„ç”¨æˆ·å/rmrb-crawler.git
   cd rmrb-crawler
   ```

2. è¿è¡Œçˆ¬è™«ï¼š

   ```bash
   python rmrb_crawler.py
   ```

3. ç”Ÿæˆçš„æ–‡ä»¶ç¤ºä¾‹ï¼š

   ```
   rmrb_20251020.txt
   ```

---

## âš™ï¸ ä¸»è¦å‚æ•°

| å‚æ•°   | åŠŸèƒ½è¯´æ˜             |
| ---- | ---------------- |
| è‡ªåŠ¨æ—¥æœŸ | è‡ªåŠ¨è¯†åˆ«å½“å¤©å¹´æœˆæ—¥        |
| å¤šçº¿ç¨‹  | é»˜è®¤çº¿ç¨‹æ•°ï¼š10ï¼Œå¯è‡ªè¡Œä¿®æ”¹   |
| è¾“å‡ºæ–‡ä»¶ | æŒ‰æ—¥æœŸå‘½åä¿å­˜ï¼ŒUTF-8 ç¼–ç  |

---

## ğŸ“ è¾“å‡ºç¤ºä¾‹

```
ã€å‘ç€å®ä¼Ÿç›®æ ‡æ¥ç»­å¥‹è¿›ã€‘

â€¦â€¦æ­£æ–‡å†…å®¹â€¦â€¦

============================================================

ã€é£é©°çš„å…‰å½±ï¼Œæ˜ ç…§è¿½æ¢¦çš„ä¸­å›½ã€‘

â€¦â€¦æ­£æ–‡å†…å®¹â€¦â€¦
```

---

## ğŸ”„ åç»­è®¡åˆ’

* [ ] æ”¯æŒæŒ‡å®šæ—¥æœŸèŒƒå›´è‡ªåŠ¨çˆ¬å–
* [ ] æ”¯æŒ PDF / Word å¯¼å‡º
* [ ] æ–­ç‚¹ç»­çˆ¬ä¸é”™è¯¯é‡è¯•

---

## ğŸ§¾ LICENSE

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE)

````

---

## ğŸ“„ `requirements.txt`

```text
requests>=2.31.0
beautifulsoup4>=4.12.2
````

---

## ğŸ“„ `.gitignore`

```gitignore
__pycache__/
*.pyc
*.txt
*.log
.env
```

---

## ğŸ“„ `LICENSE`ï¼ˆMITï¼‰

```text
MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND.
```

---

## ğŸ“„ `rmrb_crawler.py`

```python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

"""
rmrb-crawler
-------------
è‡ªåŠ¨çˆ¬å–äººæ°‘æ—¥æŠ¥ç”µå­ç‰ˆå½“æ—¥æ‰€æœ‰ç‰ˆé¢æ–‡ç« ï¼Œå¹¶åˆå¹¶ä¿å­˜ä¸ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚
"""

def get_all_sections(base_url):
    """è·å–å½“å¤©æ‰€æœ‰ç‰ˆé¢é“¾æ¥"""
    index_url = base_url + "node_01.html"
    res = requests.get(index_url)
    res.encoding = "utf-8"
    if res.status_code != 200:
        raise Exception("æ— æ³•è®¿é—®äººæ°‘æ—¥æŠ¥å½“æ—¥é¡µé¢ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ—¥æœŸã€‚")

    soup = BeautifulSoup(res.text, "html.parser")
    sections = soup.select("div.rightMenu a")

    section_links = []
    for s in sections:
        href = s.get("href")
        if href and href.startswith("node_"):
            section_links.append(urljoin(base_url, href))

    if not section_links:
        section_links = [index_url]

    print(f"ğŸ“° å…±å‘ç° {len(section_links)} ä¸ªç‰ˆé¢")
    return section_links


def get_articles_from_section(section_url):
    """è·å–ç‰ˆé¢ä¸­çš„æ–‡ç« é“¾æ¥"""
    res = requests.get(section_url)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")
    ul = soup.find("ul", class_="news-list")
    articles = []
    if not ul:
        return articles
    for a in ul.find_all("a"):
        href = a.get("href")
        title = a.get_text(strip=True)
        if href:
            full_url = urljoin(section_url, href)
            articles.append((title, full_url))
    return articles


def get_article_text(title, url):
    """ä¸‹è½½æ–‡ç« æ­£æ–‡"""
    try:
        res = requests.get(url, timeout=10)
        res.encoding = "utf-8"
        soup = BeautifulSoup(res.text, "html.parser")
        content = soup.find("div", class_="article-content") or soup.find("div", id="articleContent")
        if not content:
            return f"ã€{title}ã€‘\nï¼ˆæœªæ‰¾åˆ°æ­£æ–‡ï¼‰\n\n"
        text = content.get_text("\n", strip=True)
        return f"ã€{title}ã€‘\n\n{text}\n\n{'='*60}\n\n"
    except Exception as e:
        return f"ã€{title}ã€‘\nï¼ˆä¸‹è½½å¤±è´¥ï¼š{e}ï¼‰\n\n{'='*60}\n\n"


def main():
    today = datetime.now().strftime("%Y%m/%d")
    base_url = f"https://paper.people.com.cn/rmrb/pc/layout/{today}/"

    print(f"ğŸ“… å½“å‰æŠ“å–æ—¥æœŸ: {today}")

    sections = get_all_sections(base_url)
    all_articles = []
    for section_url in sections:
        articles = get_articles_from_section(section_url)
        all_articles.extend(articles)

    print(f"ğŸ“„ å…±å‘ç° {len(all_articles)} ç¯‡æ–‡ç« ")

    results = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_article = {executor.submit(get_article_text, t, u): (t, u) for t, u in all_articles}
        for i, future in enumerate(as_completed(future_to_article), 1):
            title, _ = future_to_article[future]
            try:
                text = future.result()
                results.append(text)
                print(f"âœ… [{i}/{len(all_articles)}] å®Œæˆ: {title}")
            except Exception as e:
                print(f"âŒ [{i}] {title} å¤±è´¥: {e}")

    out_file = f"rmrb_{datetime.now().strftime('%Y%m%d')}.txt"
    with open(out_file, "w", encoding="utf-8") as f:
        f.writelines(results)

    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ç« å·²ä¿å­˜åˆ° {out_file}")


if __name__ == "__main__":
    main()
```

